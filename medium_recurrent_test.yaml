backend:
  backend_type: "wgpu"

dataset:
  source: "tinyshakespeare"
  dataset_path: "data/tiny_shakespeare.txt"

model:
  embed_dim: 256
  pool_size: 40000        # ~10.2M parameters (40k * 256)
  num_heads: 8
  context_length: 128
  recurrence_steps: 4     # Deep reasoning mode (4 loops per token)
  router:
    type: "standard"
    hidden_dim: 128
    k_min: 64             # Active params: ~16k (0.16% active)
    k_max: 256            # Active params: ~65k (0.6% active) - Very sparse!
    exploration_noise: 0.1

training:
  batch_size: 16
  num_steps: 100          # Short run just to verify stability and generation
  learning_rate: 0.0005
  log_interval: 10
  save_interval: 50
  checkpoint_dir: "checkpoints_medium"

curriculum:
  warmup_steps: 20
  specialization_steps: 40
  warmup_epsilon: 1.0
  specialization_epsilon_start: 1.0
  specialization_epsilon_end: 0.1
  maturity_epsilon: 0.05
  balance_weight: 0.05
  efficiency_weight: 0.01
  z_loss_weight: 0.001

inference:
  default_prompt: "ROMEO:"
  max_tokens: 100
  temperature: 0.8
