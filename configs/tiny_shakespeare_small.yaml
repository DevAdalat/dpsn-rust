model:
  embed_dim: 128
  pool_size: 150000
  k_min: 50
  k_max: 4000
  router_hidden_dim: 64
  context_length: 64
  exploration_noise: 0.1
  use_hierarchical_router: true
  num_clusters: 64
  top_clusters: 8

training:
  num_steps: 700
  batch_size: 16
  learning_rate: 0.001
  log_interval: 1
  save_interval: 500
  checkpoint_dir: checkpoints

dataset:
  source: tinyshakespeare
  data_dir: data

inference:
  max_tokens: 200
  temperature: 0.8
  default_prompt: "ROMEO:"

backend:
  backend_type: wgpu

curriculum:
  warmup_steps: 100
  specialization_steps: 400
  balance_weight: 0.1
  efficiency_weight: 0.05
  z_loss_weight: 0.001

device_placement:
  pool: "Gpu"       # Keep massive parameter pool in System RAM
  router: "Gpu"     # Run routing logic on GPU
  embedding: "Gpu"  # Compute embeddings on GPU
  engine: "Gpu"     # Run attention/execution on GPU
  head: "Gpu"       # Compute final output on GPU
