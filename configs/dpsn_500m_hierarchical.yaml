model:
  vocab_size: 50257
  embed_dim: 1600
  pool_size: 200783
  num_heads: 25
  context_length: 1024
  recurrence_steps: 1
  router:
    type: hierarchical
    k_min: 1003
    k_max: 10039
    num_clusters: 64
    top_clusters: 4
    exploration_noise: 0.1

training:
  num_steps: 10000
  batch_size: 8
  learning_rate: 0.0003
  log_interval: 50
  save_interval: 1000
  checkpoint_dir: checkpoints_auto

dataset:
  source: localparquet
  parquet:
    file: "data/output.parquet"
    column: "Joke"
  tokenizer_path: "data/tokenizer.json"

inference:
  max_tokens: 500
  temperature: 0.8
  default_prompt: "The "

backend:
  backend_type: wgpu

curriculum:
  warmup_steps: 1000
  specialization_steps: 5000
  balance_weight: 0.1
  efficiency_weight: 0.05
  z_loss_weight: 0.001

device_placement:
  pool: "Cpu"
  router: "Gpu"
  embedding: "Gpu"
  engine: "Gpu"
  head: "Gpu"
