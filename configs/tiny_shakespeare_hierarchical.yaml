model:
  embed_dim: 128
  pool_size: 8000
  k_min: 50
  k_max: 400
  router_hidden_dim: 64
  context_length: 64
  num_heads: 4
  exploration_noise: 0.1
  use_hierarchical_router: true
  num_clusters: 64
  top_clusters: 8

training:
  num_steps: 5000
  batch_size: 16
  learning_rate: 0.001
  log_interval: 50
  save_interval: 500
  checkpoint_dir: checkpoints_hierarchical

dataset:
  source: tinyshakespeare
  data_dir: data

inference:
  max_tokens: 200
  temperature: 0.8
  default_prompt: "ROMEO:"

backend:
  backend_type: wgpu

curriculum:
  warmup_steps: 500
  specialization_steps: 2000
  balance_weight: 0.1
  efficiency_weight: 0.05
  z_loss_weight: 0.001
